In dieser Videoeinheit wird das Backend und die technische Architektur behandelt. Das Backend ist typischerweise ein Informationssystem, das in der Architektur des Systems im 3. Tier entworfen wird. Es beinhaltet eine Persistenzlösung wie eine Datenbank und dient als Bindeglied zwischen dem Frontend und dem Benutzer, der mit dem System arbeitet und auf Daten zugreift.

Die technische Architektur des Backends legt den Rahmen für die Implementierung der fachlichen Komponenten fest. Es gibt verschiedene Möglichkeiten, wie diese Komponenten strukturiert und integriert werden können. Eines der offenen Themen ist die Implementierung und Strukturierung der Komponenten im Backend. Dazu gehören Aufgaben wie das Anbieten von Funktionalitäten über Schnittstellen, die Implementierung der Geschäftslogik, die Verwaltung von Transaktionen, die Validierung von Daten und die Sicherstellung der Informationssicherheit.

Es ist wichtig, die Geschäftslogik im Backend zu kapseln und zu vermeiden, dass sie doppelt implementiert wird. Das Frontend kann leistungsstarke Logiken enthalten, doch einige Aufgaben müssen dennoch im Backend ausgeführt werden. Das Backend muss auch Schnittstellen sowohl für interne als auch externe Systeme bereitstellen und die Datenverwaltung sowie die Transaktionsverwaltung übernehmen. Die Sicherheit und die Validierung der Daten sind ebenfalls wichtige Aspekte, die im Backend berücksichtigt werden müssen.
Um das Thema Sicherheit geht es auch darum, was passiert, und um Fehlerbehebung. Es ist wichtig sicherzustellen, dass Fehler ordentlich behandelt werden, damit sie nicht einfach auftreten können.

Der schlimmste Fall ist, dass Fehler weitergereicht werden und das Backend aufgrund einer Ausnahme abstürzt. Neben der Fehlerbehandlung müssen auch andere Aspekte berücksichtigt werden, wie das Monitoring und Logging, die Integration von Drittsystemen und die Gewährleistung des reibungslosen Betriebs.

Es gibt viele Aufgaben im Backend, die verortet werden müssen, um Fehler zu identifizieren und Änderungen vorzunehmen. In Bezug auf Sicherheit gehört es beispielsweise zur grundlegenden Sicherheitsmaßnahme, dass eine authentifizierte Barriere errichtet wird, bevor auf das Backend zugegriffen wird.

Des Weiteren müssen Rechte überprüft werden, um sicherzustellen, dass bestimmte Aktionen nur von berechtigten Nutzern durchgeführt werden können. Es gibt datenabhängige Rechte, die die Einsicht und Bearbeitung von spezifischen Daten regeln.

Zur Sicherheit gehört auch die Verwendung von standardisierten Authentifizierungsprotokollen wie ROOS 2 oder Open ID, um die Zugriffsrechte zu verwalten.

In Bezug auf Transaktionen ist es wichtig, zwischen fachlichen und technischen Transaktionen zu unterscheiden. Technische Transaktionen müssen atomar konsistent sein und eine isolierte Durchführung garantieren.

Die Fehlerbehandlung ist auch ein wichtiger Aspekt, der sowohl das Vermeiden von Systemabstürzen als auch das Bereitstellen sinnvoller Fehlermeldungen umfasst. Manchmal können unverständliche Fehlermeldungen dazu führen, dass ein Problem nicht richtig identifiziert werden kann.
Um einen Text flüssiger zu formulieren, könnte man die Abschnitte wie folgt umschreiben:

14:38
Wenn das passiert, reagieren Experten normalerweise so, wie ich es beschrieben habe. Ein Informationssystemexperte, der gerade einen Auftrag abgeschlossen hat, wird nicht erfreut reagieren, wenn ein Fehler auftritt. Es ist üblich, dass dann Anrufe im Support eingehen. Deshalb müssen Fehler architekturell sinnvoll adressiert werden, was auch die Unterscheidung verschiedener Fehlerarten beinhaltet.

Wichtig ist hierbei die Unterscheidung zwischen spezifizierten und unspezifizierten Fehlern. Spezifiziertes Verhalten ist das, was bewusst implementiert wurde, während unspezifiziertes Verhalten zu unvorhergesehenen Fehlern führen kann, wie z.B. wenn die Datenbank nicht verfügbar ist. Es ist wichtig, spezifische Fehler als spezifische Ausnahmen zu behandeln und unspezifische Fehler als Runtime-Ausnahmen zu lassen.

Eine transparente Fehlerbehandlung ist entscheidend, um benutzerfreundliche Fehlermeldungen zu liefern. Fehler sollten erweiterbar sein, um zusätzliche Informationen, wie z.B. eine Fehler-ID für den Support, bereitzustellen. Durch eine klare Strukturierung des Backends können technische Feinheiten berücksichtigt und die Komplexität reduziert werden.

In der nächsten Vorlesungseinheit werden wir uns die Verteilung von technischen Aufgaben im Backend genauer ansehen. Dabei gibt es zwei populäre Ansätze: die Schichtenarchitektur und die Hexagonale Architektur. Beide Ansätze konzentrieren sich darauf, die Geschäftslogik klar zu strukturieren.

Die Schichtenarchitektur unterteilt das Backend in Fassaden-, Logik- und Datenzugriffsschichten, die klar definierte Aufgaben haben. Es gibt jedoch verschiedene Interpretationen darüber, welche Aufgaben in welche Schicht gehören, z.B. die Implementierung der Geschäftslogik. Hierbei kommen individuelle Entscheidungen und Projektanforderungen zum Tragen.
Um die Idee einer anderen Beschichtung zu veranschaulichen, habe ich hier eine Liste erstellt. Dies ist auch ein Bestandteil eines Tactical DWD. Ich habe das Bild hier markiert, das später noch einmal auftaucht, um das User Interface darzustellen. Natürlich gehört das nicht zum Backend, sondern zum Frontend. Das Backend ist in eine Applikationsschicht unterteilt, die den Zugriff auf die Domänenschicht ermöglicht. Die Domänenschicht beinhaltet die reine fachliche Logik und das fachliche Modell.

Die Frage, die sich stellt, ist: Welche Funktionalitäten schneidet die Praxissystem-Applikation für den Sinn der Applikation an?

Die Zugriffsschicht hält keinen Zustand oder Ähnliches. Es handelt sich tatsächlich um eine Art Zugriffschicht, wie bereits erläutert. Eine Alternative dazu wäre eine Infrastrukturschicht, die auch vom User Interface genutzt wird und technische Querschnittsdienste bereitstellt, wie die UI Library, Persistenz oder Ähnliches. Man sollte auch die Vollständigkeit berücksichtigen und beachten, dass immer klare Aufrufbedingungen vorhanden sein sollten.

Zuletzt kommen wir zur hexagonalen Architektur, die derzeit weit verbreitet und auch bei uns immer beliebter wird.

Eine zweite Möglichkeit, Aufgaben auf technischer Ebene zu verteilen, besteht darin, es nicht als Schicht zu betrachten, sondern eher als eine Schicht mit einer äußeren Schicht drum herum. Aus diesem Grund wird es oft Anion-Architektur genannt, eine Zwiebelarchitektur mit verschiedenen Schichten im Kern.

In der Geschäftslogik, die in diesem Konzept verankert ist, und die Idee hinter der hexagonalen Architektur besteht darin, das Prinzip der Trennung von Zuständigkeiten weiter zu verstärken und darauf zu bestehen, dass diese Application Core idealerweise keine Technik beinhaltet.

Der Gedanke dahinter ist, dass man immer klare Entscheidungen treffen kann. Es besteht natürlich die Möglichkeit, dies im Sinne eines Trade-offs zu variieren, je nachdem, inwieweit man dies umsetzen möchte, beispielsweise ob Spring-orientierte Technik verwendet werden soll oder nicht. Dies ist eine Frage, bei der oft gesagt wurde, es sei egal, und ähnlich wie die Nullsoftware-Kategorien genannt wurden. Heutzutage, in der Cloud-Architektur, ist dies jedoch nicht mehr der Fall, da Spring nicht so performant ist. Beispielsweise dauert es etwa zehn Sekunden, um eine Spring-Anwendung in Spring-Boot zu starten, was in einer skalierbaren Welt, in der mehrere Instanzen eines Backends in der Cloud laufen lassen, nicht mehr akzeptabel ist.

Um dies zu umgehen, wurden Lösungen wie Quarkus entwickelt, die als alternative Komponenten dienen und den Java-Code wirklich in nativen Code übersetzen, was zu Startzeiten von unter einer Sekunde oder sogar unter 100 Millisekunden führt.

Der Application Core wird dann von einer Adapterschicht umgeben, die in zwei Untergruppen unterteilt ist. Eine Gruppe sind die Primary oder Driving Adapters bzw. eingehenden Adapter, die von außen auf den Application Core zugreifen möchten, sei es über REST, WSDL, gRPC oder eine Nachricht, die empfangen wird, oder auch durch einen Batch-Steuerungsvorgang.

Die andere Gruppe sind die Secondary Driven oder Outgoing Adapters, welche die ausgehenden Aufrufe darstellen. Dies bedeutet, dass der Application Core sich selbst Daten aus anderen Systemen besorgen muss oder eine Nachricht versenden will. In diesem Fall definiert der Application Core einen Port, also eine Schnittstelle, die die Implementierung bereitstellt. Die Abhängigkeitsrichtung geht dabei immer nur von außen nach innen.

Dieser Ansatz ermöglicht eine flexiblere Gestaltung im Vergleich zur herkömmlichen Schichtenarchitektur. Man ist nicht auf Persistenz als einzige Abhängigkeit beschränkt und die Struktur kann beliebig erweitert werden.

Der Name "Hexagon" in der Hexagonalen Architektur bezieht sich lediglich auf die Darstellung. Wichtig ist zu verstehen, dass es nicht nur sechs Schnittstellen gibt, sondern beliebig viele. Die Flexibilität und klare Trennung von Zuständigkeiten bieten dem Muster im Vergleich zur Schichtenarchitektur einige Vorteile, wie auch in einem Artikel von Netflix beschrieben.

Netflix hat beispielsweise gezeigt, wie sie Geschäftslogik aus einem vorhandenen Monolithen extrahiert haben, und zwar nur die Logik, während sie weiterhin mit Adapters arbeiteten, um die Daten zu speichern und abzurufen. Später migrierten sie die Daten in eine eigene Datenbanklösung und passten nur den Adapter an, um auf einen Datenbankadapter umzusteigen und flexibel zu migrieren.

Diese Beispiele verdeutlichen die Flexibilität, die durch die klare Trennung von Zuständigkeiten erreicht wird, was später in der weiteren Backend-Entwicklung von großem Vorteil ist.

Nachdem wir nun die beiden grundlegenden Arten der Verteilung von technischen Verantwortlichkeiten, nämlich die Schichtenarchitektur und die hexagonale Architektur, besprochen haben, sehen wir uns an, wie die fachliche Strukturierung umgesetzt werden kann. Wir beginnen noch einmal mit einer einfachen Kombination mit der Schichtenarchitektur, um ein Gesamtbild der Struktur eines Backends zu erhalten. Was hier gezeigt wird, sind im Wesentlichen die einzelnen Komponenten des fachlichen Komponentenschnitts und darüber liegen die verschiedenen Schichten, darunter auch die Datenzugriffsschicht, in der das Datenmodell mit seinen Entitäten abgebildet ist, um sie abzurufen, zu speichern und zu bearbeiten.
In der Regel wird häufig ein weiteres Subjekt oder ein zusätzliches Element, das als das sogenannte Datenzugriffsobjekt (Data Access Object - DAO) bekannt ist, eingeführt. Beides werde ich gleich genauer betrachten, da es letztendlich dazu dient, die Entität zu verwalten. Darüber liegt dann die Geschäftslogik, und in diesem Use Case-getriebenen Ansatz wird die Geschäftslogik einfach in Use Cases gekapselt. Das bedeutet, ich versuche, kohärente Funktionalität in einem Artefakt zusammenzufassen, das dann eine Schnittstelle nach außen bietet. Das beinhaltet im Wesentlichen Dinge wie das Platzieren eines Auftrags, das Stornieren eines Auftrags, die Optimierung der Lagerhaltung oder die Kontrolle des Lagerbestands. All diese Dinge sind mögliche Use Cases in unseren Hochschulinformationssystemen, wie beispielsweise das Anlegen eines Praktikums, das Erstellen eines neuen Lehrplans für ein Semester, das Anlegen eines Mensaplans in der Lehrplanverwaltung und ähnliches. Das sind einzelne Teile, die typischerweise gebündelt werden. Ein Use Case-Artefakt enthält meistens mehr, da man dort versucht, die gemeinsame Funktionalität reinzubringen. Gerade bei der Verwaltung des Mensaplans beispielsweise geht es nicht nur darum, ihn anzulegen, sondern auch, ihn zu löschen, zu verändern und zu lesen.

Darüber existiert die Fassadenschicht oder Serviceschicht, teilweise auch als solche bezeichnet, in der Services als Artefakte platziert sind. Diese greifen auf die Use Cases zu und exponieren diese nach außen, in die Außenwelt. Üblicherweise ruft man nicht einfach einen Use Case auf, holt die Entitäten raus und gibt sie ans Frontend zurück, da dies letztendlich das Geheimnisprinzip verletzt und technische Probleme verursachen kann. Daher führt man sogenannte Datentransferobjekte ein, die ein eigenes Datenmodell darstellen und nach außen gegeben werden. Diese werden teilweise nur in der Fassadenschicht gemappt und genutzt, während die Geschäftslogik nur auf Entitäten basiert und darin implementiert wird.

Es gibt verschiedene Ansätze, wie die Aufgaben verteilt werden können und wie das Mappen von Entitäten zu diesen Datentransferobjekten erfolgt. Diese Unterschiede zeigen sich auch darin, wie die Datentransferobjekte modelliert werden. Nachdem wir uns das genauer angesehen haben, sind das im Wesentlichen die Schritte, um eine fachliche Komponente im Backend zu strukturieren und dies bei jeder Komponente konsistent und ähnlich umzusetzen.

Das Zusammenspiel dieser Komponenten kann anhand eines Beispiels verdeutlicht werden, wie zum Beispiel eine BG-Verwaltung für eine Wohngemeinschaft. Dort könnte ein BG-Service alle WGs finden und diese Informationen werden dann an einen BG-Wartungs-Use-Case weitergegeben, um sie abzurufen. Dieser Use Case führt dann die Datenbankabfrage durch und liefert die Ergebnisse zurück. Der WG-Service übernimmt dann letztendlich das Mapping. Dies ist ein einfaches Beispiel, es kann jedoch weitere Orchestrierungen in den Services geben sowie zusätzliche Aktionen im Wartungs-Use-Case und die Möglichkeit, andere Systeme aufzurufen.

Die Handhabung der Data Transfer Objects (DTOs) variiert auch hier je nach Vorgehensweise. Manchmal werden sie auf Use-Case-Ebene erstellt, manchmal auf Service-Ebene. In der Regel sind diese DTOs sehr ähnliche Darstellungen von Entitäten, dienen aber als Abstraktionsebene und werden konsistent verwendet. Als nächstes werden wir uns die einzelnen Artefakte auf den verschiedenen Ebenen genauer anschauen und dabei ist ersichtlich, wo wir uns jeweils befinden.

Im identitätsbezogenen Datenzugriffsschicht-Modell sind Entitäten üblicherweise reine Datenmodelle. Man findet dort keine Methoden, sondern nur Attribute und Assoziationen zu anderen Entitäten oder Kind-Entitäten. Diese Regelung wird in der Regel eingehalten, um sicherzustellen, dass dort keinerlei Geschäftslogik enthalten ist und sie wirklich als reine Datenmodelle betrachtet werden können. Dennoch haben diese Entitäten natürlich einen Lebenszyklus, können also angelegt, verändert und gelöscht werden. Zudem verfügen sie in der Regel über eine Identität, die verwendet wird, um sie zu identifizieren, sowohl auf fachlicher als auch auf technischer Ebene.

Zusätzlich zum anämischen Datenmodell gibt es auch Varianten, in denen Entitäten Methoden und damit Logik anbieten können, sofern sie sich beispielsweise auf die eigenen Attribute oder auf die Methoden von benachbarten oder assoziierten Entitäten beziehen. Dabei bleibt die Geschäftslogik jedoch auf die Entitäten beschränkt, auf welche sie angewendet wird. Diese Möglichkeit dient dazu, das Modell etwas objektorientierter zu gestalten, erfordert aber auch ein Verständnis und eine korrekte Anwendung seitens der Entwickler. Es handelt sich hierbei um eine Trade-Off-Entscheidung, wie weit man bei der Implementierung dieser Funktionen gehen möchte.

Hinsichtlich der Assoziationen zu Entitäten aus anderen Komponenten muss man überlegen, wie man damit umgehen möchte. In vielen Fällen bestehen Beziehungen zu anderen Entitäten, sei es zu Kundenobjekten oder Lagerobjekten. Typischerweise versucht man jedoch, dies nicht auf dieser Ebene zu klären, um keine enge Kopplung zu erzeugen. Ansonsten hätte man ein riesiges Datenmodell über alle fachlichen Komponenten. Man entkoppelt sie daher durch Verwendung von Referenzen, wie einer technischen ID oder einem fachlichen Schlüssel. Alternativ kann man auch die Einführung von speziellen Interfaces für Entitäten in Betracht ziehen, die beispielsweise nur Lesefunktionalitäten bietet, ohne Veränderungen zuzulassen, um die Datenhoheit zu gewährleisten. Diese Entscheidung wird getroffen, um enge Kopplungen zu vermeiden und das Risiko von unbeabsichtigten Veränderungen an den Daten zu minimieren, die möglicherweise aufgrund von Änderungen in anderen Komponenten erfolgen.
Um ehrlich zu sein, hat man völlig den Überblick verloren, wer eigentlich welche Daten ändert. Das ist ja gerade hier die Idee bei der Komponentenorientierung, dass man diese strikte Trennung und Kapselung halt einsetzt.

Gut, kommen wir zu den Artefakten. Die Frage war, wie komme ich jetzt an die Ethik ran, damit ich damit arbeiten kann, und das passiert halt über dieses Data Access Object Pattern. Da habe ich dann letztendlich einen Dao-Interface. Mit den üblichen Methoden wie zum Beispiel Find ID, Find All, Find Buy, Update und Lead. Das sind so die klassischen Methoden und das wird dann irgendwo implementiert und nutzt bei der Implementierung irgendeine Persistenz Library wie zum Beispiel Hibernate, Eclipse Link, Hibernate in dotnet-Bereich und so weiter. Das Ziel ist es bei diesem Pattern, das wirklich ein Entwurfsmuster ist, dass man die Persistenz auch austauschen könnte. Also wenn ich einen anderen Datastore habe, Low xq Datenbank vielleicht im extremen Fall oder einfach nur ein Dateisystem, wo ich irgendwie Dateien ablege, dann kann ich das mit diesem Pattern austauschbar machen, habe dann die Flexibilität, welche Lösung ich wähle.

Die Vorteile, die sich dadurch ergeben, sind wirklich die, dass man diese Flexibilität bekommt. Man hat auch eine klare Trennung von Zuständigkeiten, hier kümmert sich die Implementierung genau um die konkrete Persistenzlösung, und im Kopf kennt man das davon. Die Nachteile sind, dass der ER-Mapper ja eigentlich auch schon so etwas versucht. Wenn man jetzt eine relationale Datenbank hat, liegt genau der Punkt. Also wenn ich jetzt einen Mapper nutze und auch eine relationale Datenbank habe, dann fühlt sich das teilweise doppelt gemoppelt an, weil der ER-Mapper ermöglicht es mir, die konkrete relationale Datenbank einfach auszutauschen. Das sind aber zwei unterschiedliche Ebenen, wenn ich wirklich versuche, dann eine ganz andere Lösung dahinter zu tun. Wird es dann nicht ganz so einfach, dann würde ich auch keinen Ehre mehr einsetzen.

Dieses Data Access Object Pattern ermöglicht es auch schön, den Aspekt zu adressieren, den wir bisher noch nicht so genau angeschaut haben oder nicht im Detail angeschaut haben, nämlich diese datenabhängige Autorisierung. Das lässt sich wirklich schön verankern und auch transparent behandeln. Und das ist so eine schöne Erweiterung, finde ich.

Dort verankert in diesem Dao, dann liefert mir diese Stau, wenn ich es implementiert habe, auch nur die Entitäten zurück, die ich als angemeldeter User überhaupt sehen darf oder verändern darf. Das letztendlich die Idee, heißt ich muss das irgendwie hinkriegen, dass wenn ich hier ein Find All aufrufe oder auch ein Find by ID, dass dieses Dao prüft, ob ich dazu überhaupt berechtigt bin. Das ist die Idee und das kann man wirklich generisch implementieren.

Man kann es sogar so generisch implementieren, dass man wirklich Queries erweitert. Also wenn ich jetzt hier ein Find All habe, kann ich in dieser Dao-Implementierung automatisch eine Bedingung in der SQL-Abfrage zum Beispiel hinzufügen oder auch bei anderen Lösungen ähnlich, wo ich genau diese Suchparameter einschränke. Und aus dieser Session die erlaubten Attribute hole und das dann auf der Entität einschränke, wo es gespeichert ist. Das funktioniert so lange gut, solange man halt ein bestimmtes Set erlaubt. Wenn ein Benutzer immer nur einer Abteilung angehört, da gibt es vielleicht ein paar Super-User, die haben 10-20 Abteilungen, das ist auch noch ok. Dann ist das eine SQL-Einschränkung mit 20 Attributen, das kriegen die meisten Datenbanken dann auch noch mit entsprechenden Indizes gut hin. Schwieriger wird es dann, wenn das mehr wird.

In der Praxis ist das auch tatsächlich mal passiert. Bei einem Kunden hatten wir den Fall, 23 Jahre, nachdem wir das eingeführt haben, das System, das schon im Betrieb war, gab es eine Änderung in der Organisation, dass sie Shared Service Center eingeführt haben, das waren dann 3 Shared Service Center weltweit, die Standardaufgaben im System übernommen haben. Das heißt, das waren typischerweise dann Mitarbeiter, die hatten Rechte auf wesentlich mehr Abteilungen, auch teilweise unterschiedlichen Ländern, sogar Abteilungen, so dass dann die Query, die resultierende, plötzlich riesengroß wurde. Wenn das passiert, dann ist man dagegen natürlich nicht gewappnet, dann kriegen plötzlich die Performance in den Shared Service Centern in den Keller, hat teilweise sogar Performance-Auswirkungen auf die anderen Abteilungen gehabt. Das ist ein Beispiel, wo so eine Intransparenz halt auch. Damit rechnet man nicht mehr unbedingt, weil das lief alles. Das haben nicht alle immer auf dem Schirm, und plötzlich merken, ah ja, das gab es ja auch noch, diese generische Implementierung und dann muss man halt entsprechend tätig werden und eine andere Lösung finden oder die Lösung erweitern. Wir haben das auch in den Griff bekommen, aber das ist mal so ein Beispiel aus der Praxis, wo so etwas im Hintergrund passiert, auch vielleicht mal unerwartete Probleme in der Produktion im Betrieb verursachen kann.
Wir sind nun beim eigentlichen Kern angelangt, den Muse Cases, in denen die Geschäftslogik implementiert ist. Ein Beispiel verdeutlicht das Prinzip dahinter.

Use Cases sind im Grunde eine Bündelung von Geschäftslogik in einem Faktoren namens Use Case, der typischerweise zusammengehörige Operationen mit hoher Kohäsion vereint. Beispielsweise könnte es um das Anlegen eines Auftrags gehen oder das Anlegen eines Auftrags mit Standardeinstellungen für den Benutzer oder den Kunden. Indem man diese logischen Operationen in einem Use Case bündelt, stellt man sicher, dass fachliche Regeln, die zusammengehören, auch dort implementiert sind. Dies erfordert eine enge Abstimmung mit dem Fachbereich, um gemeinsame Änderungen vornehmen zu können oder in Beziehung zu stehen. 

Eine wichtige Aufgabe ist es auch, den Use Case gut zu strukturieren und zu schneiden, was häufig nicht präzise genug gemacht wird. Oftmals liegt der Fokus zu sehr auf den Daten, was möglicherweise nicht dem besten Vorgehen entspricht. Es ist ratsam, das Bündeln ähnlich wie der Fachbereich anzugehen, um optimale Schnittpunkte zu finden.

Es ist wichtig, dass ein Use Case keine technischen Abhängigkeiten nutzt. Durch die DAO-Schnittstelle bleibt dieser frei von technischen Details und kann Entitätsobjekte abrufen. Die Nutzung von Use Cases aus anderen Komponenten oder Services ermöglicht eine dynamische Gestaltung, besonders in sich dynamisch ändernden Geschäftsszenarien, ohne die Implementierung jedes Mal anpassen zu müssen.

Es ist möglich, Regeln in einer domänenspezifischen Sprache konfigurierbar im System abzulegen und in Echtzeit zu ändern. Beispielsweise können Entscheidungsbäume oder Entscheidungstabellen modelliert werden, um verschiedene Bedingungen zur Laufzeit zu prüfen und entsprechende Aktionen auszulösen. Diese dynamischen Anpassungen können die Effizienz und Flexibilität eines Systems erhöhen.

Diese Vorgehensweise erleichtert die Strukturierung von Use Cases und die Implementierung von Geschäftslogik, was insgesamt Entwickler unterstützt und den Prozess effizienter gestaltet. Die Suche nach dem richtigen Schnitt für Use Cases ist eine weitere Herausforderung, aber durch die Fokussierung auf Datenmodelle und die Implementierung der Logik wird die Herangehensweise klarer und einfacher.

Abschließend werden die Services in der Fassadenschicht betrachtet, die eine dedizierte Schnittstelle für verschiedene Clients oder Backends bieten. Ein Service enthält üblicherweise eine transparente Zugriffsschicht, die Sicherheitsüberprüfungen, Transaktionsverwaltung und Netzwerkanbindungen umfasst. Durch Frameworks wird die technische Integration auf der Netzwerkebene erleichtert, während die eigentliche Logik in den Use Cases implementiert wird. Es ist entscheidend, den Service gut zu konfigurieren, um Expressivität und Effizienz zu gewährleisten.
In diesem Fall wird in dem Update Kundeninformationen editiert. Ein Objekt wird hinzugefügt und vereinfacht dargestellt.

Dann erfolgt die Delegation und die Anwendungsfälle, die entsprechend abgeschlossen sind. Man kann auch die Fehlerbehandlung transparent gestalten, indem man Exception-Handler definiert, nicht wahr?

Und auch beschränkt auf bestimmte Ausnahmen, wenn sie auftreten. Diese Exception wird dann aktiv und ermöglicht eine transparente Fehlerbehandlung. Man kann eine generische Umwandlung vornehmen, ein RID erzeugen und das RID protokollieren.

Üblicherweise wird bereits im Voraus sichergestellt, dass eine Corelation-ID für das Logging vorhanden ist, die immer mitgeloggt wird und auch an dieser Stelle zurückgegeben wird, was bedeutet, dass sie in der Fehlermeldung an den Client zurückgegeben wird und so kann ein Service dort aussehen. Dazu gehört auch eine gewisse Konfiguration, nicht nur in Bezug auf die Datenbank, die ich jetzt nicht aufgeführt habe, sondern auch in Bezug auf die Sicherheit, wie man die Sicherheit gewährleisten kann, das wird auch hier mit einem Konfigurationsstrahl wie Configuration Beam durchgeführt. Das ist auch ein Java Beam, The Configuration Assistant, im Spring Framework bekannt.

Dort kann man zum Beispiel mit Enable Global Method Security genau das hier aktivieren, dass es möglich ist, mit diesem Pre-Authorized zu arbeiten, durch Web Security aktiviert man generell die Sicherheit, und dann kann man dort eine Sicherheitskonfiguration definieren.

Indem man von einem standardmäßigen Adapter erbt und dort entsprechend eine Methode überschreibt, die die konfigurierte Methode überschreibt, die HTTP-Sicherheit erweitert.

Und da kann man dann beispielsweise für eine große API für HTTP mit dem Sitzungsmanagement sagen, dass man es zustandslos halten möchte und man auch Anforderungen autorisiert und zwar alle, und sagt, alle brauchen mindestens die Berechtigung "Display".

Ansonsten wird jeder Request abgelehnt und man kann auch beispielsweise dort einfache Konfigurationen vornehmen. Es gehört noch ein wenig mehr dazu im Hintergrund, aber im Allgemeinen sieht man, dass es eine relativ einfache Konfiguration programmatisch ermöglicht, die notwendig ist, um eine Sicherheitskonfiguration zu erstellen und letztendlich diesen HTTP-Zugriff, der über Spring geregelt wird, zu sichern, um sicherzustellen, dass nichts Dummes passiert, solange zumindest ein Spring Container stabil ist.

Das war also ein Überblick über all diese verschiedenen Begriffe und damit ihr ein entsprechendes Bild habt.

Nun kommen wir zu dem letzten relevanten Artefakt, nämlich den Datenmodellen. Die Idee hinter den DTOs besteht üblicherweise darin, von den konkreten Datenmodellen innerhalb der Komponenten abzusehen und tatsächlich benutzerorientierte Schnittstellen sauber aufzubereiten. für diejenigen, die es nutzen, mit einem eigenen Datenmodell, sodass man nicht immer das gesamte interne Datenmodell nach außen preisgeben muss und nicht weiß, was andere damit anfangen. Das ist zu viel Wissen, das nach außen definiert wird.

Neben der Schnittstelle eines Dienstes, den man vielleicht nutzt, ist auch das DTO wirklich eine Abstraktion und ein wichtiger Teil der Schnittstelle. Das Datenmodell, das nach außen gegeben werden soll, ist im Wesentlichen wieder die Entität, nichts anderes als ein reiner Datencontainer, der auch dem Datentransfer dient. Daher sollte bei einem DTO beispielsweise berücksichtigt werden, dass die Daten serialisierbar sein sollten. Das ist vielleicht nicht überall der Fall, wenn man komplexe Daten mit Character Lobs aus relationalen Datenbanken oder ähnlichem hat, dann kann das problematisch sein, je nachdem, was man verwendet, um die Daten auf die Leitung zu bringen. Im Falle einer REST-API könnte man einen JSON-Mapper verwenden und möglicherweise Annotationen nutzten, um dies signalisierbar zu machen, und diesen Aspekt dieser Aufgabe kann man in das DTO verlagern.

Ein DTO sollte also keine Logik enthalten, auch keine Methoden. Das Frontend kann diese schließlich nicht nutzen, es sei denn, es handelt sich um eine nativ Java Methoden-Schnittstelle. Mit RMI-Javapatch denke ich. Wirklich Objekte übergeben können kann, das ist tatsächlich möglich, aber dafür benötigt man auch die gleiche Klasse auf der anderen Seite und hat auch andere Implikationen. Also im Allgemeinen macht Logik in einem DTO keinen Sinn, das ist ganz klar und es sollte auch wenige Abhängigkeiten zu irgendwelchen Diensten oder Anwendungsfällen haben, sondern sich wirklich nur auf DTOs beziehen und entweder vollständig oder gar nicht befüllt sein, beziehungsweise im DTO sollte klar definiert sein, welche Attribute der Nutzer erwarten kann, dass diese noch gesetzt sind, damit man wirklich weiß, was man in der Hand hat.

Hier noch einmal kurz zusammengefasst: Warum sollte man nicht einfach die Entität nach außen geben, neben dem Aspekt, dass man die Entkopplung möchte? Das ist der Punkt, nicht wahr? Man möchte wirklich die Schnittstellen der Komponenten von der Umwelt entkoppeln und wenig Wissen nach außen geben, und auch die Datenhoheit behalten. Ein sehr wichtiger Punkt beim DTO sind vielleicht Setter-Aufrufe oder ähnliches, das die Komponente, aus der es stammt, eigentlich nicht interessiert. Bei der Entität hatten wir das Prinzip der Datenhoheit, bei den DTOs kann es dann leicht dazu führen, dass es auch in der Datenbank persistiert wird, und man auch irgendwie volle Kontrolle darüber hat, was nach außen geht. Über einen Datentransfer kann man dadurch auch ein Stück weit steuern, dass nicht zu viele Daten transferiert werden, die irrelevant sind oder ähnliches.

Man kann auch wirklich die Persistenz im Blick haben, also gerade, wenn eine Entität von Halbernet gemanagt wird, die solche Dinge wie Lazy Collections enthalten kann, also Listen von assoziierten anderen Entitäten, die erst dann aufgelöst werden, wenn man darauf zugreift.

Dafür wird dann transparent ein Proxy vorgeschaltet, die das ermöglichen, das sieht der Entwickler nicht. Das sind versteckte Details, die aber dann spätestens Probleme verursachen können, wenn sie übertragen werden, weil vielleicht die Serialisierung nach Abschluss der Transaktion passiert und man dort keine Daten abfragen kann. Das sind einige der Gründe, weshalb man dies so machen möchte, und hier kommt es nun darauf an, wie eigentlich das DTO ist und.

Es gibt im Grunde zwei verschiedene Herangehensweisen, zwei extreme, würde ich sagen, von einem Trade-off her, nämlich ein reines Datenmodell-orientiertes, bei dem eine Entität gleich einem DTO ist. Man hat da ein direktes Mapping der Attribute, hat dadurch eine klare Sicht auf die Daten, allerdings auch wenig Kontrolle.

Auf der anderen Seite hat man auch wenig Aufwand dahinter. Das Ganze kann man relativ einfach generieren und hat trotzdem Abstraktion. Im Notfall kann man immer noch bei der Entität etwas anderes machen und das DTO so belassen. Man hat dann diese Freiheit, das heißt, die Entkopplung ist auf jeden Fall eher gegeben. Es geht eher in Richtung expliziter Schnittstellen, wenn ihr euch erinnert bei den Schnittstellenarten, wo man dann.
Grundlegend wird für jeden Anwendungsfall eine spezifische deutsche Betriebssystemumgebung wirklich spezifisch definiert, maßgeschneidert und leistungsstark. Es wird nur das bereitgestellt, was für den Anwendungsfall wirklich benötigt wird, möglicherweise auch mit komplexeren Zuordnungen, was ein explizites Design erfordert und natürlich auch mit einem gewissen Mehraufwand verbunden ist.

Es gibt einen Versuch, eine Mischform davon zu schaffen, die als Data Transfer Structures (DTS) bezeichnet wird.

Es gibt auch sogenannte Composite DTOS, also CTOS oder CDTOS. Ich habe schon alles gesehen, das ist eine Art Mittelweg. Wenn z. B. das Modell einen Kunden mit einer Adresse als zweites Merkmal hat, dann gibt es 2 DTOS: einen Customer-DTO und einen Address-DTO, und das Customer Address-DTS verknüpft die beiden und sorgt so für eine 1:1-Zuordnung. Man kann auch ein Teilmenge von den Daten der Identität wählen.

Auf jeden Fall sollte man nicht die Assoziationsanalytik implementieren, das ergibt theoretisch keinen Sinn.

Die DTS können auch selbst abgeleitete Attribute enthalten, was eine ziemliche Flexibilität bietet, die man wirklich nutzen sollte. Ich persönlich bevorzuge den datenmodellorientierten Ansatz, da es gefährlich ist, einfach wegen des Tendenz zur Datentummen die Entitäten immer weiter wachsen zu lassen und die DTOs zu erweitern, obwohl dies für einige Anwendungsfälle gar nicht relevant ist.

Das Design eines Datenmodells für die Schnittstelle ist eine explizite Designaufgabe.

Für einfache Komponenten wie Stammdaten kann dies relativ einfach sein, jedoch sollte man wirklich darüber nachdenken, anstatt einfach stumpf zu generieren, ohne mitzudenken. Ich habe tatsächlich Fälle gesehen, in denen riesige DTS irgendwo in eine andere Komponente übergeben wurden, die schon auf oberster Ebene allein 10 Attribute enthielten, wovon 3 sogar Verknüpfungen zu weiteren DTS waren.

Ein Entwickler, den ich mal begleitet habe, benötigte tatsächlich nur ein einzelnes Attribut auf Ebene 3 oder 4 und ein Attribut auf oberster Ebene, um den Datenmodell zu verstehen. Es macht zwar irgendwie funktionieren, aber nicht so, wie man es sich wünschen würde, da zu viel Wissen nach außen diffundiert. Es fehlt an einer wirklich losen Kopplung.

Als Schlussfolgerung vom Case-Driven-Ansatz gibt es den Schichtenarchitekturansatz, der häufig verwendet wird. Ich würde jedoch kritisch hinterfragen, ob dies tatsächlich sinnvoll ist.

Während des Designs versuche ich, die Komponenten zu identifizieren, indem ich sie zusammen gruppiere und die Abhängigkeiten zwischen ihnen festlege, um letztendlich zu einem Komponentendesign zu gelangen, das auf fachlichen Datenmodellen oder Ideen basiert.

Man muss immer prüfen, ob die Zuweisung Sinn ergibt und ob die Verantwortlichkeiten klar beschrieben sind. Die Abhängigkeiten dienen als letzte Validierung, ob alles funktioniert.

Eine sehr wichtige Überlegung wäre, ob die dynamische Laufzeit gut funktioniert und ob die Hauptanwendungsfälle reibungslos ablaufen. Das führt zu einem datenzentrierten Entwurf, der stark von den Datenmodellen beeinflusst wird. Im Gegensatz dazu ist strategisches DDD flexibler und prozessorientierter.

Es kann sein, dass dasselbe Datenobjekt in unterschiedlichen Kontexten und mit verschiedenen Modellen mehrmals verwendet wird.
Um 1:07:27 herum wird in der Praxis immer deutlicher, dass dies der bessere Ansatz ist. Dennoch möchte ich euch auch hier eine alternative Möglichkeit vorstellen, die von einigen praktiziert wurde und wird. Es ist wichtig, dies kritisch zu hinterfragen, da es schnell und einfach sein kann, es auf diese Weise zu machen.

In der bevorstehenden Präsenzveranstaltung werden wir möglicherweise in der nächsten Woche oder vielleicht auch in der übernächsten Woche ein Beispiel anschauen, bei dem der Service und das Becken verwendet werden. Dieses Beispiel basiert auf unserem Framework Devener B, das wir Open Source zur Verfügung gestellt haben. Es implementiert die Architektur, die ich erläutert habe, grob. Die Verantwortlichkeiten sind klarer definiert und strenger, und es wird viel Logik in die Geschäftsabläufe verlagert, die auch die GTOs definiert und Transaktionen sowie Rechte verwaltet.

Es handelt sich um eine Interpretation der Schichtenarchitektur, und wir haben ein Beispiel für eine Anwendung erstellt, in diesem Fall eine Restaurantverwaltung. Wir werden uns einige dieser Bausteine gemeinsam anschauen, den Code durchgehen und Fragen stellen können, um ein reales Beispiel zu sehen.

Das war erst einmal eine kurze Zusammenfassung. Nun haben wir uns den Use Case angesehen. Es bleiben noch einige technische Aspekte zu Strukturierungsmöglichkeiten im Backend offen, insbesondere das taktische Design des Domain-Driven Designs. Eric Evans hat verschiedene Muster definiert, die angewendet werden können, auch bekannt als Building Blocks.

Dieses Konzept basiert darauf, dass in einem Kontext eine eindeutige Sprache existieren soll. Dies ermöglicht eine klare Kommunikation innerhalb dieses Kontextes, sodass jeder z.B. weiß, was mit einem Kunden oder einem Auftrag gemeint ist. Die Idee des Modells besteht darin, diese interne Sprache in einem modernen objektorientierten Modell auszudrücken, das aus verschiedenen Elementen wie Services und Entitäten besteht.

Darüber hinaus umfasst es unter anderem auch Repositories, die Dinge ähneln oder äquivalent zu den DAOs sind, die wir zuvor gesehen haben, sowie Aggregates als Kapselung und Wurzeln von Entitäten. Aggregates sind meiner Meinung nach lange Zeit unterschätzt worden und bündeln im Wesentlichen mehrere Entitäten.

Zudem gibt es Wertobjekte, die unveränderlich sind und meistens nur aus Werten bestehen. Diese dienen dazu, Entitäten und hauptsächlich Aggregate zu erstellen.

Diese Elemente bilden im Wesentlichen das taktische Design im Domain-Driven Design. In dem Ursprungsdokument war auch die technische Unterstrukturierung des Backends vorgesehen. Interessant ist hier das alternative Konzept einer Smart UI, bei dem die Geschäftslogik im Backend gekapselt wird und die Smart UI lediglich die Funktionalitäten des Backends abbildet.

Wir werden uns diese Muster nun im Detail anschauen. Wir beginnen mit den Entitäten, die sich von den Entitäten im use case-basierten Ansatz unterscheiden. In diesem Konzept entsprechen Entitäten einem Objekt in der fachlichen Domäne und sind identifizierbar. Der Hauptunterschied besteht darin, dass die Geschäftslogik in den Entitäten liegt, d.h. Entitäten bieten Methoden an und beinhalten Value Objects als Attribute.

Value Objects dienen der Beschreibung eines Objekts und haben keinen eigenen Lebenszyklus. Wenn sich beispielsweise im vorherigen Beispiel die Informationen des Vertriebskontakts eines Kunden ändern, muss ein neues Value Object erstellt und aktualisiert werden. Value Objects sind Teil der Entität und beschreiben einen fachlichen Datentyp.

Das sind im Wesentlichen die Grundelemente des Domain-Driven Designs.
Um 1:15:20, hat der Sprecher erwähnt, dass das Value-Objekt fachliche Logik enthalten kann, wie beispielsweise Validierungen. Es können keine Änderungen vorgenommen werden, jedoch können Informationen über das Value-Objekt ausgewertet und preisgegeben werden.

Er fügt hinzu, dass ein Value-Objekt aus einem Boundlet-Kontext in einem anderen Boundlet-Kontext existieren kann, entweder als Entität oder Wertobjekt, je nachdem, wie es in verschiedene Kontexte eingebunden wird.

Auf 1:16:31 führt der Sprecher ein Beispiel mit einer Buchausleihe in einer Bücherei an, wo ein Buch in einem Katalog als Entität oder Wertobjekt gesehen werden kann, je nach Kontext.

Zu 1:17:07 im Kontext des RAKs werden Beispiele wie Adresse, Kontonummer, Ratenposition usw. genannt, die als Entität oder Wertobjekt betrachtet werden können, je nach Funktion und Verwendungszweck.

Schließlich erklärt der Sprecher um 1:17:53 die Bedeutung von Aggregaten in Bezug auf Entitäten und Value-Objekte, die eine übergeordnete Strukturierungskomponente darstellen und fachliche Logiken bündeln. Aggregates repräsentieren eine Art Wurzel-Entität und definieren eine Transaktionsklammer, um Entitäten in einen konsistenten Zustand zu überführen.

Der Sprecher erwähnt auch, dass Aggregates sorgfältig im Hinblick auf fachliche Anforderungen und in Absprache mit dem Fachbereich entworfen werden müssen. Um Aggregates zu identifizieren, kann sogar Design-Level Eventstorming verwendet werden.

Zusätzlich zu Aggregates werden auch Repositories, Services und Factories erläutert als Elemente, die den Zugriff auf Entitäten ermöglichen, die Orchestrierung von Logiken unterstützen und die Erstellung von Entitäten oder Aggregaten vereinfachen. Die Konsistenz und Korrektheit der Regeln werden durch Patterns wie das Builder Pattern sichergestellt.
Um die Zusammenhänge klarer zu zeigen, schließt sich hier also der Kreis zum ersten Teil. Hier wird eine Möglichkeit aufgezeigt, wie man die verschiedenen Elemente auf destactical.de DS sehen kann: die Aggregaten, die Entitäten und die Factories. Es gibt auch noch die Domäne Events, fachliche Ereignisse, die man modellieren kann. Der Domain Core, also die Kernkomponente, beinhaltet die Dinge innerhalb des Modells, und es können auch Domain Services angeboten werden. Das wird hier vereinfacht dargestellt.

Dienste nach außen, die Applikationsservices, sind tatsächlich Teil der Adapterschicht und stellen eine Art Import-Tor dar, über das sie auf Aggregates oder Domain Services zugreifen können.

Die Repositories sind Implementierungen eines Output-Ports eines ausgehenden Adapters, über den beispielsweise konkrete Datenbanklösungen angeboten werden. Außerdem gibt es einen Messaging-Adapter, über den Daten versendet werden. Fachliche Ereignisse sind ein weiteres Beispiel dafür.

Auf diese Weise kann der Domain Core wirklich mit einem umfangreichen objektorientierten Modell modelliert werden, bei dem die Domain-Sprache gemeinsam mit den Fachbereichen entwickelt wird.

Es ist wichtig, ein grundlegendes Verständnis dafür zu haben. Auf diese Weise kann man ein gutes, passgenaues Design finden, das sowohl für den Fachbereich als auch für die IT verständlich ist und eine solide Grundlage bietet, um Änderungen passgenau vorzunehmen. Es reflektiert im Grunde die Denkweise des Fachbereichs und bietet eine Ideallösung. Natürlich erfordert es einige Iterationen, bis man dieses Ziel erreicht. Man wird nicht von heute auf morgen dort sein.

Ein Beispiel aus dem Buch "Hands-On Domain-Driven Design by Example" von Michael Blöd wird eingebunden, dabei handelt es sich um eine Kreditapplikation für eine Bank, bei der ein Kunde sich für einen Immobilienkredit bewirbt und eine Bewertung sowie ein Scoring für den Kunden durchgeführt werden, um zu bestimmen, ob er diesen Kredit erhält. Hier werden wir uns das gemeinsam ansehen.

Zum Abschluss kommen wir zur Zusammenfassung des Backend-Entwurfs. Es gibt zwei grundlegende Entscheidungen, die getroffen werden müssen. Erstens, wie wird das Backend strukturiert und wie werden die technischen Aufgaben in Unterelemente verteilt, sei es durch Schichtenarchitektur oder hexagonalen Ansatz. Zweitens, wie wird die Fachlichkeit strukturiert, hier gibt es verschiedene Möglichkeiten.

Eine Möglichkeit ist der Use Case-driven Ansatz, bei dem man einfach die Use Cases definiert und mit dem reinen Datenmodell arbeitet. Das kann für Entwickler leicht verständlich sein, erfordert jedoch dennoch etwas Entwurfsarbeit, um die Wartbarkeit sicherzustellen. Eine objektorientierte Modellierung erfordert dagegen mehr Aufwand, kann aber in bestimmten Fällen sinnvoller sein.

Besonders bei differenzierenden Core-Subdomänen, wo Schnelligkeit und Qualität gefragt sind, macht die objektorientierte Modellierung Sinn. In solchen Fällen kann auch eine Kombination mit einer hexagonalen Architektur sinnvoll sein, um die Zuständigkeiten klar zu trennen.

Das war eine Zusammenfassung zum Kapitel Backend-Entwurf. Als nächstes werden wir uns mit der Persistenz eine Ebene tiefer beschäftigen.
